<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Learned sensing: jointly optimized microscope hardware for accurate image classification | Deep Learning in Imaging</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Learned sensing: jointly optimized microscope hardware for accurate image classification" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Computational Optics Lab" />
<meta property="og:description" content="Computational Optics Lab" />
<link rel="canonical" href="http://localhost:4000/learned_sensing_dnn" />
<meta property="og:url" content="http://localhost:4000/learned_sensing_dnn" />
<meta property="og:site_name" content="Deep Learning in Imaging" />
<script type="application/ld+json">
{"@type":"WebPage","url":"http://localhost:4000/learned_sensing_dnn","headline":"Learned sensing: jointly optimized microscope hardware for accurate image classification","description":"Computational Optics Lab","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Deep Learning in Imaging" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Deep Learning in Imaging</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/learned_sensing/">Learned Sensing</a><a class="page-link" href="/group_projects/">Group Projects</a><a class="page-link" href="/class_projects/">Class Projects</a><a class="page-link" href="/related_work/">Related Work</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Learned sensing: jointly optimized microscope hardware for accurate image classification</h1>
  </header>

  <div class="post-content">
    <h4 id="alex-muthumbi1amey-chaware2-kanghyun-kim2-kevin-zhou3-pavan-chandra-konda3-richard-chen4-benjamin-judkewitz5-andreas-erdmann61-barbara-kappes7-roarke-horstmeyer23">Alex Muthumbi<sup>1,+</sup>,Amey Chaware<sup>2,+</sup>, Kanghyun Kim<sup>2</sup>, Kevin Zhou<sup>3</sup>, Pavan Chandra Konda<sup>3</sup>, Richard Chen<sup>4</sup>, Benjamin Judkewitz<sup>5</sup>, Andreas Erdmann<sup>6,1</sup>, Barbara Kappes<sup>7</sup>, Roarke Horstmeyer<sup>2,3</sup></h4>
<p><em>1: School of Advanced Optical Technologies, Friedrich-Alexander University, Erlangen, Germany.</em> <br />
<em>2: Department of Electrical and Computer Engineering, Duke University, Durham NC, USA.</em>  <br />
<em>3: Department of Biomedical Engineering, Duke University, Durham NC, USA.</em>  <br />
<em>4: Y Combinator Research, San Francisco, CA.</em>  <br />
<em>5: NeuroCure Cluster of Excellence, Charite Universitatsmedizin and Humboldt University, Berlin, Germany.</em>  <br />
<em>6: Fraunhofer IISB, Erlangen, Germany.</em>  <br />
<em>7: Department of Chemical and Biological Engineering, Friedrich-Alexander University, Erlangen, Germany.</em>  <br />
<em>+ denotes equally contributing authors</em></p>

<h4 id="abstract">Abstract</h4>
<p>Since its invention, the microscope has been optimized for interpretation by a human observer. With the recent development of deep learning algorithms for automated image analysis, there is now a clear need to re-design the microscope’s hardware for specific interpretation tasks. To increase the speed and accuracy of automated image classification, this work presents a method to co-optimize how a sample is illuminated in a microscope, along with a pipeline to automatically classify the resulting image, using a deep neural network. By adding a ‘‘physical layer’’ to a deep classification network, we are able to jointly optimize for specific illumination patterns that highlight the most important sample features for the particular learning task at hand, which may not be obvious under standard illumination. We demonstrate how our learned sensing approach for illumination design can automatically identify malaria-infected cells with up to 5-10% greater accuracy than standard and alternative microscope lighting designs. We show that this joint hardware-software design procedure generalizes to offer accurate diagnoses for two different blood smear types, and experimentally show how our new procedure can translate across different experimental setups while maintaining high accuracy.</p>

<p><img src="/assets/images/microscope.png" alt="Our setup" /></p>
<div><small>Figure 1: We present a learned sensing network (LSN), which optimizes a microscope's illumination to improve the accuracy of automated image classification. (a) Standard optical microscope outfitted with an array of individually controllable LEDs for illumination. (b) Network training is accomplished with a large number of training image stacks, each containing Ns uniquely illuminated images. The proposed network's physical layer combines images within a stack via a weighted sum before classifying the result, where each weight corresponds to the relative brightness of each LED in the array. (c) After training, the physical layer returns an optimized LED illumination pattern that is displayed on the LED array to improve classification accuracies in subsequent experiments.</small></div>
<p><br /><br /></p>

<h4 id="results">Results</h4>
<center>
<img src="/assets/images/lsdnn-table1.png" alt="Thin Smear Results" />
<br /><br />

<img src="/assets/images/lsdnn-fig2.png" alt="Thin Smear Images" />
<div><small>Figure 2: Example images of individual thin-smear blood cells under different forms of illumination. Top two rows are negative examples of cells that do not include a malaria parasite, bottom two rows are positive examples that contain the parasite. Example illuminations include from (a) just the center LED, (b) uniform light from all LEDs, (c) all LEDs with uniformly random brightnesses, (d) a phase contrast-type (PC) arrangement, (e) an off-axis LED, (f) a phase contrast (PC) ring, (g) optimized pattern with red illumination, (h) optimized multispectral pattern, and (i) the same as in (h) but showing response to each individual LED color in pseudo-color, to highlight color illumination’s effect at different locations across the sample.</small></div>
<br /><br />

<img src="/assets/images/lsdnn-table2.png" alt="Thick Smear Results" />
<br /><br />

<img src="/assets/images/lsdnn-fig3.png" alt="Thick Smear Images" />
<div><small>Figure 3: Example images of individual thick-smear blood cells under different forms of illumination. Top two rows are negative examples of cells that do not include a malaria parasite, bottom two rows are positive examples that contain the parasite. Example illuminations include from (a) just the center LED, (b) uniform light from all LEDs, (c) all LEDs with uniformly random brightnesses, (d) a phase contrast-type (PC) arrangement, (e) an off-axis LED, (f) a phase contrast (PC) ring, (g) optimized pattern with red illumination, (h) optimized multispectral pattern, and (i) the same as in (h) but showing response to each individual LED color in pseudo-color, to highlight color illumination’s effect at different locations across the sample.</small></div>
<br /><br />
</center>

<h4 id="code-and-data">Code and Data</h4>
<p><a href="/projects/lsdnn/example_CNN_code.zip">Code</a></p>

<p><a href="/projects/lsdnn/data.zip">Data</a></p>

<p><a href="/projects/lsdnn/README.txt">README</a></p>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Deep Learning in Imaging</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Deep Learning in Imaging</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Computational Optics Lab</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
